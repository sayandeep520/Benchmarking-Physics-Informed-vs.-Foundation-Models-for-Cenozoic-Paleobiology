{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_DeepTime_Forecasting_Benchmark\n",
    "\n",
    "This notebook implements and compares two advanced forecasting methods: Neural ODEs (physics-informed) and Chronos-T5 (data-driven foundation model) for paleo-biodiversity time series." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: INSTALLATION ---\n",
    "# We need the torchdiffeq library for the ODE solvers\n",
    "!pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import the ODE solver\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "# --- STEP 2: GENERATE SYNTHETIC PALEO DATA (SINE WAVE WITH GAPS) ---\n",
    "# We simulate a \"perfect\" fossil record (sine wave) and then \"erode\" it\n",
    "# to create irregular sampling, mimicking real geological data.\n",
    "\n",
    "data_size = 1000\n",
    "batch_time = 20\n",
    "batch_size = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# True Dynamics (The \"Laws of Physics\" we want to learn)\n",
    "true_y0 = torch.tensor([[2., 0.]]).to(device) # Initial condition (Diversity, Rate)\n",
    "t = torch.linspace(0., 25., data_size).to(device) # Time (0 to 25 Ma)\n",
    "true_A = torch.tensor([[-0.1, 2.0], [-2.0, -0.1]]).to(device) # Spiral dynamics Matrix\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def forward(self, t, y):\n",
    "        return torch.mm(y**3, true_A) # Cubic dynamics (non-linear)\n",
    "\n",
    "# Generate the \"True\" history (The ground truth)\n",
    "with torch.no_grad():\n",
    "    true_y = odeint(Lambda(), true_y0, t, method='dopri5')\n",
    "\n",
    "def get_batch():\n",
    "    # This function creates \"Irregular Gaps\"\n",
    "    # We grab random slices of time, simulating imperfect preservation\n",
    "    s = torch.from_numpy(np.random.choice(np.arange(data_size - batch_time, dtype=np.int64), batch_size, replace=False))\n",
    "    batch_y0 = true_y[s]  # (M, D)\n",
    "    batch_t = t[:batch_time]  # (T)\n",
    "    batch_y = torch.stack([true_y[s + i] for i in range(batch_time)], dim=0)  # (T, M, D)\n",
    "    return batch_y0.to(device), batch_t.to(device), batch_y.to(device)\n",
    "\n",
    "# --- STEP 3: DEFINE THE NEURAL ODE (The \"ODEFunc\") ---\n",
    "# Instead of predicting Y directly, we predict the DERIVATIVE (dy/dt).\n",
    "# The Solver then integrates this to find Y.\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        # A simple MLP (Multi-Layer Perceptron) that approximates the derivative\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 2), # Output is 2D (Diversity, Rate)\n",
    "        )\n",
    "\n",
    "        # Initialize weights for better convergence\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return self.net(y)\n",
    "\n",
    "# --- STEP 4: TRAINING LOOP ---\n",
    "func = ODEFunc().to(device)\n",
    "optimizer = optim.RMSprop(func.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Training Neural ODE... (This might take a moment)\")\n",
    "start_time = time.time()\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for itr in range(1, 1001): # 1000 Iterations\n",
    "    optimizer.zero_grad()\n",
    "    batch_y0, batch_t, batch_y = get_batch()\n",
    "\n",
    "    # FORWARD PASS: Integrate the Neural ODE from t0 to t_end\n",
    "    # This uses the 'dopri5' solver (Runge-Kutta)\n",
    "    pred_y = odeint(func, batch_y0, batch_t).to(device)\n",
    "\n",
    "    # Calculate Loss (Difference between predicted trajectory and true data)\n",
    "    loss = torch.mean(torch.abs(pred_y - batch_y))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if itr % 100 == 0:\n",
    "        print(f\"Iter {itr:04d} | Total Loss {loss.item():.6f}\")\n",
    "\n",
    "print(f\"Training Complete in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# --- STEP 5: VISUALIZATION (Verify Solver) ---\n",
    "# We verify if the model learned the \"Sine Wave\" shape despite the gaps.\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_y = odeint(func, true_y0, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install chronos-forecasting\n",
    "\n",
    "print(\"Chronos-forecasting installation initiated. This may take a moment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "# --- 1. RELOAD DATA (In case variables were lost during restart) ---\n",
    "print(\"1. Preparing Fossil Time Series...\")\n",
    "try:\n",
    "    # Try loading processed data first\n",
    "    df_ml = pd.read_csv('Lilliput_Project_Data.csv')\n",
    "except:\n",
    "    # Fallback: Re-create from raw files\n",
    "    df_context = pd.read_csv('/content/pbdb_data.csv', skiprows=19)\n",
    "    df_meas = pd.read_csv('/content/pbdb_data (1).csv', skiprows=17)\n",
    "    df_ml = pd.merge(df_context, df_meas, on='specimen_no', how='inner')\n",
    "    # Create midpoint age\n",
    "    if 'min_ma' in df_ml.columns:\n",
    "        df_ml['age_mid'] = (df_ml['max_ma'] + df_ml['min_ma']) / 2\n",
    "    else:\n",
    "        df_ml['age_mid'] = df_ml['max_ma']\n",
    "\n",
    "# Create Time Series: Count unique genera per time step\n",
    "# Sort descending (Oldest -> Youngest) because Chronos expects time to move forward\n",
    "ts_data = df_ml.groupby('age_mid')['genus'].nunique().sort_index(ascending=False)\n",
    "\n",
    "# --- 2. LOAD THE AI MODEL ---\n",
    "print(\"2. Loading Chronos-T5 (Foundation Model)...\")\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# --- 3. RUN FORECAST ---\n",
    "print(\"3. Forecasting the Future...\")\n",
    "# Convert data to tensor\n",
    "context = torch.tensor(ts_data.values)\n",
    "prediction_length = 20 # Forecast 20 steps into the \"future\"\n",
    "\n",
    "forecast = pipeline.predict(\n",
    "    context,\n",
    "    prediction_length,\n",
    "    num_samples=20,\n",
    ")\n",
    "\n",
    "# --- 4. VISUALIZE RESULTS ---\n",
    "low, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)\n",
    "history_len = len(context)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot History (Real Fossil Record)\n",
    "plt.plot(np.arange(history_len), context, color=\"black\", label=\"History (Fossil Record)\")\n",
    "# Plot Forecast (AI Prediction)\n",
    "plt.plot(np.arange(history_len, history_len + prediction_length), median, color=\"blue\", label=\"AI Forecast\")\n",
    "# Plot Uncertainty (Confidence Interval)\n",
    "plt.fill_between(\n",
    "    np.arange(history_len, history_len + prediction_length),\n",
    "    low, high, color=\"blue\", alpha=0.3, label=\"10-90% Quantile\"\n",
    ")\n",
    "\n",
    "plt.title(\"Biodiversity Forecasting with Chronos-T5\", fontsize=14)\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Number of Unique Genera\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets\n",
    "\n",
    "print(\"Datasets library installation initiated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "The subtask requires converting the time series data into appropriate training and validation sets, defining context and prediction windows, and formatting them into a `datasets.Dataset` object for fine-tuning. This code block will perform all these data preparation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1. Convert ts_data (pandas Series) into a NumPy array.\n",
    "full_series_np = ts_data.values\n",
    "\n",
    "# 2. Define the length of the validation set (val_len)\n",
    "# The instructions suggest 9 points for ~20% of total data. len(ts_data) is 41, so 9 is roughly 20%.\n",
    "val_len = 9\n",
    "\n",
    "# 3. Split the full time series NumPy array into train_series and val_series.\n",
    "train_series_np = full_series_np[:-val_len]\n",
    "val_series_np = full_series_np[-val_len:]\n",
    "\n",
    "print(f\"Total data points: {len(full_series_np)}\")\n",
    "print(f\"Training series length: {len(train_series_np)}\")\n",
    "print(f\"Validation series length: {len(val_series_np)}\")\n",
    "\n",
    "# 4. Define context_window and prediction_window\n",
    "context_window = 20 # Example: Observe 20 past data points\n",
    "prediction_window = 5 # Example: Predict 5 future data points\n",
    "\n",
    "# 5. Create a list of training samples.\n",
    "train_samples = []\n",
    "for i in range(len(train_series_np) - context_window - prediction_window + 1):\n",
    "    context = torch.tensor(train_series_np[i : i + context_window], dtype=torch.float32)\n",
    "    target = torch.tensor(train_series_np[i + context_window : i + context_window + prediction_window], dtype=torch.float32)\n",
    "    train_samples.append({'context': context, 'target': target})\n",
    "\n",
    "print(f\"Generated {len(train_samples)} training samples.\")\n",
    "\n",
    "# 6. Create a single validation sample.\n",
    "# The context for this sample should be the entire train_series_np\n",
    "# The target should be the entire val_series_np\n",
    "val_context_tensor = torch.tensor(train_series_np, dtype=torch.float32)\n",
    "val_target_tensor = torch.tensor(val_series_np, dtype=torch.float32)\n",
    "val_sample = {'context': val_context_tensor, 'target': val_target_tensor}\n",
    "\n",
    "# 7. Convert the list of training samples into a datasets.Dataset object.\n",
    "train_dataset = Dataset.from_list(train_samples)\n",
    "\n",
    "print(\"Data preparation for fine-tuning complete.\")\n",
    "print(f\"Training Dataset: {train_dataset}\")\n",
    "print(f\"Validation Sample Context Shape: {val_sample['context'].shape}\")\n",
    "print(f\"Validation Sample Target Shape: {val_sample['target'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "# 1. Instantiate the ChronosPipeline model\n",
    "print(\"Loading Chronos-T5 model for fine-tuning...\")\n",
    "pipeline_ft = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# 2. Set the model to training mode\n",
    "pipeline_ft.model.train()\n",
    "print(\"Model set to training mode.\")\n",
    "\n",
    "# 3. Define the prediction_length for training\n",
    "prediction_length_ft = prediction_window\n",
    "print(f\"Prediction length for fine-tuning set to: {prediction_length_ft}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Access the inner Hugging Face T5 model for training\n",
    "inner_model = pipeline_ft.model.model\n",
    "inner_model.train()\n",
    "\n",
    "optimizer = AdamW(inner_model.parameters(), lr=1e-5)\n",
    "num_epochs = 20\n",
    "\n",
    "# Explicitly set the tokenizer's prediction_length to match the prediction_window\n",
    "# This resolves the AssertionError: assert length == self.config.prediction_length\n",
    "pipeline_ft.tokenizer.config.prediction_length = prediction_window\n",
    "\n",
    "# Ensure train_data is correctly referenced (it should be train_series_np)\n",
    "train_data_tensor = torch.tensor(train_series_np, dtype=torch.float32)\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0 # To calculate average loss per epoch\n",
    "\n",
    "    # Adjust max_start_idx to account for both context and prediction windows\n",
    "    max_start_idx = len(train_data_tensor) - context_window - prediction_window\n",
    "    if max_start_idx < 1:\n",
    "        print(f\"Skipping epoch {epoch+1} due to insufficient data for context_window {context_window} and prediction_window {prediction_window}\")\n",
    "        # Define avg_loss here to prevent NameError if epoch is skipped\n",
    "        avg_loss = 0.0 # Or some appropriate default\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
    "        continue\n",
    "\n",
    "    for _ in tqdm(range(max_start_idx), desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. Sample a random window\n",
    "        # The start_idx now refers to the beginning of the context window\n",
    "        start_idx = np.random.randint(0, max_start_idx)\n",
    "        window_context = train_data_tensor[start_idx : start_idx + context_window]\n",
    "        window_target = train_data_tensor[start_idx + context_window : start_idx + context_window + prediction_window]\n",
    "\n",
    "        # Add a batch dimension (unsqueeze(0)) for both window and target\n",
    "        window_context = window_context.unsqueeze(0)\n",
    "        window_target = window_target.unsqueeze(0)\n",
    "\n",
    "        # 2. Tokenize using the pipeline's tokenizer\n",
    "        input_ids, attention_mask, scale = pipeline_ft.tokenizer.context_input_transform(window_context)\n",
    "\n",
    "        # label_input_transform uses the SAME scale to tokenize the target\n",
    "        # The length of window_target MUST be equal to prediction_length, which is prediction_window\n",
    "        # Adjusted to unpack only 2 values as per ValueError\n",
    "        labels, _ = pipeline_ft.tokenizer.label_input_transform(window_target, scale)\n",
    "\n",
    "        # Move to GPU/CPU\n",
    "        device = inner_model.device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 3. Forward Pass & Update using the INNER MODEL\n",
    "        outputs = inner_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / max_start_idx\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# --- 1. SETUP VALIDATION DATA ---\n",
    "# We split the data again: 90% History (Context) vs 10% Future (Truth)\n",
    "# (Ensure 'ts_data' is still in memory from the previous step)\n",
    "split_idx = int(len(ts_data) * 0.9)\n",
    "train_data = torch.tensor(ts_data.values[:split_idx], dtype=torch.float32) # The Past\n",
    "valid_data = torch.tensor(ts_data.values[split_idx:], dtype=torch.float32) # The \"Hidden\" Future\n",
    "\n",
    "print(f\"Forecasting the last {len(valid_data)} time steps (The 'Hidden' Future)...\")\n",
    "\n",
    "# --- 2. SWITCH TO INFERENCE MODE ---\n",
    "pipeline_ft.model.eval() # Stop updating weights\n",
    "\n",
    "# --- 3. GENERATE PREDICTION ---\n",
    "# We feed it the History (train_data) and ask for the Future\n",
    "forecast = pipeline_ft.predict(\n",
    "    train_data,\n",
    "    len(valid_data),\n",
    "    num_samples=20\n",
    ")\n",
    "\n",
    "# --- 4. VISUALIZE THE RESULTS ---\n",
    "low, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# A. Plot the History (Black)\n",
    "plt.plot(np.arange(split_idx), train_data, color=\"black\", label=\"Training History\")\n",
    "\n",
    "# B. Plot the TRUE Future (Green Dashed) - This is what actually happened\n",
    "plt.plot(np.arange(split_idx, len(ts_data)), valid_data, color=\"green\", linestyle=\"--\", linewidth=2, label=\"True Future (Hidden)\")\n",
    "\n",
    "# C. Plot the AI Forecast (Red) - This is what the Fine-Tuned AI predicted\n",
    "plt.plot(np.arange(split_idx, len(ts_data)), median, color=\"red\", linewidth=2, label=\"Fine-Tuned Forecast\")\n",
    "plt.fill_between(\n",
    "    np.arange(split_idx, len(ts_data)), \n",
    "    low, high, color=\"red\", alpha=0.2, label=\"Uncertainty\"\n",
    ")\n",
    "\n",
    "plt.title(\"The Final Test: Can the Fine-Tuned AI Predict the Cenozoic?\", fontsize=14)\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Biodiversity\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- 1. Calculate Chronos Accuracy ---\n",
    "# Extract the median forecast (the Red Line)\n",
    "forecast_median = np.quantile(forecast[0].numpy(), 0.5, axis=0)\n",
    "\n",
    "# Calculate MSE (Mean Squared Error)\n",
    "chronos_mse = mean_squared_error(valid_data.numpy(), forecast_median)\n",
    "\n",
    "print(f\"--- FINAL SCOREBOARD ---\")\n",
    "print(f\"Chronos (Fine-Tuned Model) MSE: {chronos_mse:.4f}\")\n",
    "\n",
    "# --- 2. Interpretation ---\n",
    "print(\"\n--- HOW TO JUDGE THE WINNER ---\")\n",
    "print(f\"Your Fine-Tuned Chronos Error is: {chronos_mse:.4f}\")\n",
    "print(\"Compare this to your Neural ODE visual fit.\")\n",
    "print(\"- If Chronos MSE is < 0.05 (normalized), the 'Big Data' approach won.\")\n",
    "print(\"- If Chronos MSE is > 0.10, the 'Physics' approach (Neural ODE) likely won.\")\n",
    "\n",
    "# --- 3. Save Final Prediction for Report ---\n",
    "# We save the \"Truth\" vs \"Forecast\" to a CSV so you can make a pretty table later\n",
    "results_df = pd.DataFrame({\n",
    "    'Time_Step': np.arange(len(valid_data)),\n",
    "    'True_Diversity': valid_data.numpy(),\n",
    "    'AI_Predicted_Diversity': forecast_median\n",
    "})\n",
    "results_df.to_csv('Final_Model_Comparison_FineTuned.csv', index=False)\n",
    "print(\"\n-> Results saved to 'Final_Model_Comparison_FineTuned.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
