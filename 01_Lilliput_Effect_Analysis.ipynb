{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_Lilliput_Effect_Analysis\n",
    "\n",
    "This notebook covers the initial data loading, merging, feature engineering, Random Forest modeling, and visualization related to the Lilliput Effect." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the Context File (Time & Rocks)\n",
    "# 'skiprows' might vary, but based on your file, 19 is usually safe for the \"Specimens\" download\n",
    "df_context = pd.read_csv('/content/pbdb_data.csv', skiprows=19)\n",
    "\n",
    "# 2. Load the Measurement File (Sizes)\n",
    "# Based on your upload, the header is at line 17\n",
    "df_meas = pd.read_csv('/content/pbdb_data (1).csv', skiprows=17)\n",
    "\n",
    "# 3. Merge them together (The Magic Step)\n",
    "# This matches the ID number from both files\n",
    "df_final = pd.merge(df_context, df_meas, on='specimen_no', how='inner')\n",
    "\n",
    "# 4. Preview your Complete Dataset\n",
    "print(\"Success! We have merged Time, Lithology, and Size.\")\n",
    "print(df_final[['genus', 'max_ma', 'lithology1', 'measurement_type', 'average']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load Your Uploaded Climate Data ---\n",
    "print(\"1. Loading Climate Data (TableS33.tab)...\")\n",
    "\n",
    "# The file uses tabs (\\t) and has 91 lines of metadata before the table starts.\n",
    "# We skip the metadata and use the standard CENOGRID columns.\n",
    "try:\n",
    "    df_climate = pd.read_csv(\n",
    "        '/content/TableS33.tab',\n",
    "        sep='\\t',\n",
    "        skiprows=91  # Skips the text header to get to the real data\n",
    "    )\n",
    "\n",
    "    # Rename columns to match our project standard\n",
    "    # 'Tuned time [Ma]' -> age_ma\n",
    "    # 'Foram benth \\u03b418O [\\u2030 PDB] (VPDB CorrAdjusted)' -> d18o (Temperature proxy)\n",
    "    # 'Foram benth \\u03b413C [\\u2030 PDB] (VPDB CorrAdjusted)' -> d13c (Carbon proxy)\n",
    "    df_climate = df_climate.rename(columns={\n",
    "        'Tuned time [Ma]': 'age_ma',\n",
    "        'Foram benth \\u03b418O [\\u2030 PDB] (VPDB CorrAdjusted)': 'd18o',\n",
    "        'Foram benth \\u03b413C [\\u2030 PDB] (VPDB CorrAdjusted)': 'd13c'\n",
    "    })\n",
    "\n",
    "    # Filter for Cenozoic only (0-66 Ma)\n",
    "    df_climate = df_climate[(df_climate['age_ma'] >= 0) & (df_climate['age_ma'] <= 66)]\n",
    "\n",
    "    print(f\"   -> Success! Loaded {len(df_climate)} climate data points.\")\n",
    "    print(f\"   -> Time Range: {df_climate['age_ma'].min():.2f} to {df_climate['age_ma'].max():.2f} Ma\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   -> Error loading file: {e}\")\n",
    "\n",
    "# --- 2. The Binning Process (Aligning Fossils to Climate) ---\n",
    "# Ensure you have 'df_final' (your fossil data) from the previous step.\n",
    "if 'df_final' in locals() and 'df_climate' in locals():\n",
    "    print(\"\n2. Aligning Fossils with Climate...\")\n",
    "\n",
    "    # A. Calculate Fossil Midpoint Age\n",
    "    if 'min_ma' in df_final.columns:\n",
    "        df_final['age_mid'] = (df_final['max_ma'] + df_final['min_ma']) / 2\n",
    "    else:\n",
    "        df_final['age_mid'] = df_final['max_ma']\n",
    "\n",
    "    # B. Define Time Bins (0.1 Ma slices)\n",
    "    # This creates buckets like 45.0, 45.1, 45.2 Ma\n",
    "    bins = np.arange(0, 66.1, 0.1)\n",
    "    labels = bins[:-1] + 0.05\n",
    "\n",
    "    # C. Assign Fossils to Time Bins\n",
    "    df_final['time_bin'] = pd.cut(df_final['age_mid'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # D. Bin climate data and merge d18o into df_final\n",
    "    df_climate['time_bin'] = pd.cut(df_climate['age_ma'], bins=bins, labels=labels, right=False)\n",
    "    # Taking the mean d18o for each bin\n",
    "    df_climate_binned = df_climate.groupby('time_bin', observed=False)['d18o'].mean().reset_index()\n",
    "\n",
    "    # Merge climate data (d18o) into the fossil dataframe\n",
    "    # Using a left merge to keep all fossils, and add d18o where time_bins match\n",
    "    df_final = pd.merge(df_final, df_climate_binned, on='time_bin', how='left')\n",
    "\n",
    "    print(f\"   -> Successfully merged climate data (d18o) into df_final.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PHASE 2: FEATURE ENGINEERING ---\n",
    "# This converts your raw data into a matrix of numbers for the AI.\n",
    "\n",
    "# 1. Create a fresh copy for Machine Learning\n",
    "df_ml = df_final.copy()\n",
    "\n",
    "# 2. Filter for Realism (Remove bad data)\n",
    "# We need rows that have both a Body Size (average) and a Temperature (d18o)\n",
    "df_ml = df_ml.dropna(subset=['average', 'd18o'])\n",
    "df_ml = df_ml[df_ml['average'] > 0] # Size must be positive\n",
    "\n",
    "# 3. Geography: Calculate Absolute Latitude\n",
    "# We care about \"Distance from Equator\" (0 to 90), not North/South (-90 to +90).\n",
    "# PBDB usually stores this in 'lat' or 'paleolat'.\n",
    "if 'paleolat' in df_ml.columns:\n",
    "    df_ml['latitude_abs'] = df_ml['paleolat'].abs()\n",
    "elif 'lat' in df_ml.columns:\n",
    "    df_ml['latitude_abs'] = df_ml['lat'].abs()\n",
    "else:\n",
    "    print(\"Warning: Latitude column missing. Using 0.\")\n",
    "    df_ml['latitude_abs'] = 0\n",
    "\n",
    "# 4. Taphonomy: One-Hot Encoding (The \"Rock Type\" Translation)\n",
    "# This converts \"Sandstone\" into [1, 0, 0] and \"Shale\" into [0, 1, 0]\n",
    "\n",
    "# A. Clean the text (lowercase, remove punctuation)\n",
    "df_ml['lith_clean'] = df_ml['lithology1'].astype(str).str.lower().str.replace('"', '').str.strip()\n",
    "\n",
    "# B. Simplify: Only keep the Top 10 most common rocks (Group others as \"other\")\n",
    "top_10_rocks = df_ml['lith_clean'].value_counts().nlargest(10).index\n",
    "df_ml['lith_simple'] = df_ml['lith_clean'].apply(lambda x: x if x in top_10_rocks else 'other')\n",
    "\n",
    "# C. Convert to Numbers (Get Dummies)\n",
    "lith_dummies = pd.get_dummies(df_ml['lith_simple'], prefix='lith')\n",
    "df_ml = pd.concat([df_ml, lith_dummies], axis=1)\n",
    "\n",
    "# --- 5. Final Check ---\n",
    "print(\"Feature Engineering Complete!\")\n",
    "print(f\"Final Dataset Size: {len(df_ml)} rows\")\n",
    "print(\"Columns ready for AI:\", ['d18o', 'latitude_abs'] + list(lith_dummies.columns))\n",
    "\n",
    "# Show the 'Translated' Data\n",
    "print(df_ml[['genus', 'average', 'd18o', 'lith_sandstone', 'lith_shale']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# --- 1. Define Predictors (X) and Target (y) ---\n",
    "# Target: The average body size of the animal\n",
    "y = df_ml['average']\n",
    "\n",
    "# Predictors: Temperature (d18o), Geography (latitude), and Rock Type (lith_*)\n",
    "# Explicitly select only the numerical and one-hot encoded features\n",
    "predictor_columns = [\n",
    "    'd18o',\n",
    "    'latitude_abs',\n",
    "    'lith_claystone',\n",
    "    'lith_grainstone',\n",
    "    'lith_limestone',\n",
    "    'lith_marl',\n",
    "    'lith_not reported',\n",
    "    'lith_other',\n",
    "    'lith_packstone',\n",
    "    'lith_sandstone',\n",
    "    'lith_shale',\n",
    "    'lith_siliciclastic',\n",
    "    'lith_siltstone'\n",
    "]\n",
    "X = df_ml[predictor_columns]\n",
    "\n",
    "# (Optional) Verify we only have numbers\n",
    "print(\"Predictor Columns:\", X.columns.tolist())\n",
    "\n",
    "# --- 2. Split Data (Train vs. Test) ---\n",
    "# We keep 20% of the data hidden to test the model's accuracy later\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 3. Train the Random Forest ---\n",
    "print(\"Training the model... (This might take a moment)\")\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- 4. Evaluate Performance ---\n",
    "# How well can we predict body size? (1.0 is perfect, 0.0 is random guessing)\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"Model Accuracy (R^2 Score): {score:.4f}\")\n",
    "\n",
    "# --- 5. THE RESULTS (Feature Importance) ---\n",
    "# This is the answer to your Research Question.\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\n--- RESEARCH RESULTS: What drives Body Size? ---\")\n",
    "print(importances)\n",
    "\n",
    "# --- 6. Interpretation Helper ---\n",
    "top_feature = importances.iloc[0]['Feature']\n",
    "print(f\"\nCONCLUSION: The most dominant factor driving body size is '{top_feature}'.\")\n",
    "\n",
    "if 'd18o' in top_feature:\n",
    "    print(\"-> RESULT: ECOLOGICAL SIGNAL DETECTED. Climate (Temperature) is the main driver.\")\n",
    "    print(\"   (This supports the 'Lilliput Effect' hypothesis).\")\n",
    "elif 'latitude_abs' in top_feature:\n",
    "    print(\"-> RESULT: GEOGRAPHIC SIGNAL DETECTED. Latitude is the main driver.\")\n",
    "    print(\"   (This could relate to latitudinal diversity gradients or climate zones).\")\n",
    "else:\n",
    "    print(\"-> RESULT: TAPHONOMIC/OTHER SIGNAL DETECTED. A lithology or other factor is the main driver.\")\n",
    "    print(\"   (This might indicate preservation bias or other environmental controls).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Prepare Data for Plotting ---\n",
    "# Group by Time Bin to get the global average size per 0.1 Ma\n",
    "# This removes the \"noise\" of individual fossils and shows the global trend\n",
    "plot_data = df_ml.groupby('time_bin', observed=False)[['average', 'd18o']].mean().reset_index()\n",
    "\n",
    "# Convert time_bin back to numeric Age (midpoint)\n",
    "# The 'time_bin' column already contains the numeric midpoint value after pd.cut with labels\n",
    "plot_data['age_ma'] = plot_data['time_bin']\n",
    "\n",
    "# Drop empty bins\n",
    "plot_data = plot_data.dropna()\n",
    "\n",
    "# --- 2. Create the Dual-Axis Plot ---\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# A. Plot Body Size (The Target)\n",
    "color_size = 'tab:blue'\n",
    "ax1.set_xlabel('Time (Million Years Ago)')\n",
    "ax1.set_ylabel('Mean Body Size (mm)', color=color_size, fontsize=12, fontweight='bold')\n",
    "ax1.plot(plot_data['age_ma'], plot_data['average'], color=color_size, linewidth=2, label='Body Size')\n",
    "ax1.tick_params(axis='y', labelcolor=color_size)\n",
    "ax1.invert_xaxis() # Geologists always plot time from Right (Old) to Left (Young)\n",
    "\n",
    "# B. Plot Temperature (The Predictor)\n",
    "# We use a second Y-axis (right side)\n",
    "ax2 = ax1.twinx()\n",
    "color_temp = 'tab:red'\n",
    "ax2.set_ylabel('Global Temperature Proxy (d18O)', color=color_temp, fontsize=12, fontweight='bold')\n",
    "# Note: Higher d18O means COLDER. We invert this axis so UP = WARMER\n",
    "ax2.plot(plot_data['age_ma'], plot_data['d18o'], color=color_temp, linestyle='--', alpha=0.6, label='Temperature (d18O)')\n",
    "ax2.tick_params(axis='y', labelcolor=color_temp)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# --- 3. Formatting ---\n",
    "plt.title('The Lilliput Effect: Global Body Size vs. Climate (Cenozoic Era)', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PHASE 5: EXPORT RESULTS ---\n",
    "\n",
    "# 1. Save the processed data (for your report/appendix)\n",
    "df_ml.to_csv('Lilliput_Project_Data.csv', index=False)\n",
    "print(\"-> Saved dataset to 'Lilliput_Project_Data.csv'\")\n",
    "\n",
    "# 2. Save the Feature Importance (The Proof)\n",
    "# (Re-calculating importance if 'importances' variable was lost, otherwise just saves it)\n",
    "if 'importances' in locals():\n",
    "    importances.to_csv('Research_Results_Ranking.csv', index=False)\n",
    "    print(\"-> Saved feature ranking to 'Research_Results_Ranking.csv'\")\n",
    "else:\n",
    "    print(\"Warning: 'importances' variable not found. Run the Random Forest cell again.\")\n",
    "\n",
    "# 3. Save the Graph\n",
    "# We re-generate the plot briefly to ensure it saves correctly\n",
    "import matplotlib.pyplot as plt\n",
    "if 'plot_data' in locals():\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.set_xlabel('Time (Ma)')\n",
    "    ax1.set_ylabel('Body Size (mm)', color='tab:blue')\n",
    "    ax1.plot(plot_data['age_ma'], plot_data['average'], color='tab:blue', label='Size')\n",
    "    ax1.invert_xaxis()\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Temperature (d18O)', color='tab:red')\n",
    "    ax2.plot(plot_data['age_ma'], plot_data['d18o'], color='tab:red', linestyle='--', label='Temp')\n",
    "    ax2.invert_yaxis()\n",
    "\n",
    "    plt.title('Final Result: Body Size vs Climate')\n",
    "    plt.savefig('Lilliput_Discovery_Plot.png', dpi=300)\n",
    "    print(\"-> Saved graph to 'Lilliput_Discovery_Plot.png'\")\n",
    "    plt.close() # Close to prevent double plotting\n",
    "else:\n",
    "    print(\"Warning: Run the Visualization cell first!\")\n",
    "\n",
    "print(\"\nDONE! You can now download these 3 files from the Files tab on the left.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('Lilliput_Discovery_Plot.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
